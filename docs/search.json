[
  {
    "objectID": "qmd/ezpz/ezpz.html",
    "href": "qmd/ezpz/ezpz.html",
    "title": "Starting Up Distributed Training",
    "section": "",
    "text": "Application Startup Time\n\n\n\n\n\n\nFrom Tanima:\n\nHi Sam and Corey,\nThanks for your comments on measuring the application start up time last week.\nTypically, we report the throughput performance after the start-up and warm-up during the “steady” state of the training.\nWe have a few follow-up questions so that we establish a methodology to address the issue brought up by Argonne.\n\nWe can set a few timestamps in the model scripts and job scripts used for the queue submission: Job script:\nTime stamp A:  \n&lt;actual python command using mpiexec&gt;\n\nInside the model script:  \nmain()  \nTimestamp B:  \n[...]\nTimestamp C:  \nFirst training steps and onwards.  \nBy startup time, do you mean measuring time difference between A and C or B and C?\n\n\n\n\nWill the measurement methodology be the same for distributed training?\nFor examples, we can measure the start-up time for the rank0?\n\n\n\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases?\nFor example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\nJob script:\nTime stamp A:\n&lt;actual python command using mpiexec&gt;\n\nTime stamp B:\n import torch\nTime stamp C\nimport IPEX\nTime stamp D\nEtc...\nIf you have any other scenarios, please feel free to suggest.\n\nThanks, Tanima.\n\n\n\n\n\n\nIn Measuring / Calculating Startup Time,I provide a summary of how the startup time is identified and calculated.\nI’m not sure exactly I understand\n\nWill the measurement methodology be the same for distributed training? For examples, we can measure the start-up time for the rank0?\n\nThe startup time is being measured for distributed training (logs only created on RANK = 0)\nI discuss in Minimal Working Example a minimal example that can be used to measure the startup times.\n\nThis is using a library I’ve been working on, ezpz that is designed to help simplify the process of setting up / initializing distributed training across many GPUs.\n\n\n\n\n\nThe startup timing was identified by parsing the logfiles from existing runs and calculating the difference \\delta t = t_{1} - t_{0},\n\nt_{0} is the time stamp at the very beginning of the shell script (defined here) which then launches mpiexec &lt;mpi-args&gt; python3 [...].\n\nt_{0} appears in the logfile as:\nJob started at: 2023-11-02-183323 on x3004c0s13b0n0\n\nt_{1} is identified as the timestamp associated with the completion of the first training step\n\nt_{1} appears in the logfile as:\n[2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n\nBelow is an example of the bash script use to parse the logfiles and identify these timestamps:\n  $ for f in $(tail -5 logfiles) ; do echo $f; cat $f | grep -E \"Job started|step=0\\,\" | uniq ; echo \"\\n\" ; done\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-183323.log\n  Job started at: 2023-11-02-183323 on x3004c0s13b0n0\n  [2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184240.log\n  Job started at: 2023-11-02-184240 on x3015c0s37b0n0\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184259.log\n  Job started at: 2023-11-02-184259 on x3015c0s37b0n0\n  [2023-11-02 18:43:23,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-184407.log\n  Job started at: 2023-11-02-184407 on x3004c0s13b0n0\n  [2023-11-02 18:44:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb2/logs/foremans-x3108c0s25b1n0-nhosts2-ngpu8-2023-11-02-192739.log\n  Job started at: 2023-11-02-192739 on x3108c0s25b1n0\n\n\n\n\n\n\n\n Startup Times (Perlmutter)\n\n\n\n\n\n\n\nTable 1: ?(caption)\n\n\n\n\n(a) Startup times on Perlmutter\n\n\n\n\n\n\n\n\n\n\n\n\n****\nmodel_size\nworld_size\nstart\nstop\nt0\nt1\ndt\n\n\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191101.log\nGPT1T_1L\n8\n2023-10-05-191101\n2023-10-05-191215\n191101\n191215\n114\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191400.log\nGPT1T_1L\n8\n2023-10-05-191400\n2023-10-05-191511\n191400\n191511\n111\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191707.log\nGPT1T_1L\n8\n2023-10-05-191707\n2023-10-05-191817\n191707\n191817\n110\n\n\nforemans-nid008553-nhosts2-ngpu8-2023-10-15-114506.log\nGPT1T_2L\n8\n2023-10-15-114506\n2023-10-15-114616\n114506\n114616\n110\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-133531.log\nGPT2_7B\n8\n2023-10-15-133531\n2023-10-15-133745\n133531\n133745\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-135041.log\nGPT2_7B\n8\n2023-10-15-135041\n2023-10-15-135255\n135041\n135255\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-140806.log\nGPT2_7B\n8\n2023-10-15-140806\n2023-10-15-141236\n140806\n141236\n430\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-143120.log\nGPT2_7B\n8\n2023-10-15-143120\n2023-10-15-143655\n143120\n143655\n535\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154337.log\nGPT2_7B\n8\n2023-10-15-154337\n2023-10-15-154446\n154337\n154446\n109\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154943.log\nGPT1T_1L\n8\n2023-10-15-154943\n2023-10-15-155317\n154943\n155317\n374\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-162315.log\nGPT1T_1L\n8\n2023-10-15-162315\n2023-10-15-162441\n162315\n162441\n126\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-180714.log\nGPT2_7B\n8\n2023-10-15-180714\n2023-10-15-180805\n180714\n180805\n91\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-181733.log\nGPT2_7B\n8\n2023-10-15-181733\n2023-10-15-181834\n181733\n181834\n101\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-182228.log\nGPT1T_1L\n8\n2023-10-15-182228\n2023-10-15-183031\n182228\n183031\n803\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-183345.log\nGPT1T_2L\n8\n2023-10-15-183345\n2023-10-15-183750\n183345\n183750\n405\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-184442.log\nGPT1T_2L\n8\n2023-10-15-184442\n2023-10-15-184727\n184442\n184727\n285\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-185952.log\nGPT1T_1L\n8\n2023-10-15-185952\n2023-10-15-190046\n185952\n190046\n4094\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-191508.log\nGPT2_7B\n8\n2023-10-15-191508\n2023-10-15-191608\n191508\n191608\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-192404.log\nGPT2_7B\n8\n2023-10-15-192404\n2023-10-15-192504\n192404\n192504\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193041.log\nGPT2_7B\n8\n2023-10-15-193041\n2023-10-15-193137\n193041\n193137\n96\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193448.log\nGPT2_7B\n8\n2023-10-15-193448\n2023-10-15-193540\n193448\n193540\n92\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-195802.log\nGPT1T_1L\n16\n2023-10-15-195802\n2023-10-15-195904\n195802\n195904\n102\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200019.log\nGPT2_7B\n16\n2023-10-15-200019\n2023-10-15-200258\n200019\n200258\n239\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200902.log\nGPT2_7B\n16\n2023-10-15-200902\n2023-10-15-201239\n200902\n201239\n337\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201524.log\nGPT2_7B\n16\n2023-10-15-201524\n2023-10-15-201612\n201524\n201612\n88\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201834.log\nGPT2_7B\n16\n2023-10-15-201834\n2023-10-15-201923\n201834\n201923\n89\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202402.log\nGPT2_7B\n16\n2023-10-15-202402\n2023-10-15-202501\n202402\n202501\n99\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202606.log\nGPT2_7B\n16\n2023-10-15-202606\n2023-10-15-202713\n202606\n202713\n107\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084033.log\nGPT1T_1L\n8\n2023-10-16-084033\n2023-10-16-084212\n84033\n84212\n179\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084628.log\nGPT1T_1L\n8\n2023-10-16-084628\n2023-10-16-084728\n84628\n84728\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-085401.log\nGPT1T_1L\n8\n2023-10-16-085401\n2023-10-16-085505\n85401\n85505\n104\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-090142.log\nGPT1T_1L\n8\n2023-10-16-090142\n2023-10-16-090305\n90142\n90305\n163\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-093404.log\nactCkpt_GPT13B\n8\n2023-10-16-093404\n2023-10-16-093504\n93404\n93504\n100\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-101437.log\nGPT1T_1L\n16\n2023-10-16-101437\n2023-10-16-101549\n101437\n101549\n112\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-101512.log\nGPT1T_1L\n16\n2023-10-16-101512\n2023-10-16-101615\n101512\n101615\n103\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102217.log\nactCkpt_GPT25B\n16\n2023-10-16-102217\n2023-10-16-102452\n102217\n102452\n235\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102750.log\nactCkpt_GPT25B\n16\n2023-10-16-102750\n2023-10-16-103243\n102750\n103243\n493\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-103113.log\nactCkpt_GPT25B\n16\n2023-10-16-103113\n2023-10-16-103237\n103113\n103237\n124\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104037.log\nactCkpt_GPT25B\n16\n2023-10-16-104037\n2023-10-16-104148\n104037\n104148\n111\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104819.log\nactCkpt_GPT25B\n16\n2023-10-16-104819\n2023-10-16-110002\n104819\n110002\n5183\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-110119.log\nactCkpt_GPT25B\n16\n2023-10-16-110119\n2023-10-16-110225\n110119\n110225\n106\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-113715.log\nactCkpt_GPT25B\n16\n2023-10-16-113715\n2023-10-16-113824\n113715\n113824\n109\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114236.log\nGPT1T_1L\n16\n2023-10-16-114236\n2023-10-16-114338\n114236\n114338\n102\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114610.log\nGPT1T_1L\n16\n2023-10-16-114610\n2023-10-16-114711\n114610\n114711\n101\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114819.log\nGPT1T_2L\n16\n2023-10-16-114819\n2023-10-16-114953\n114819\n114953\n134\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-131058.log\nGPT1T_2L\n16\n2023-10-16-131058\n2023-10-16-131203\n131058\n131203\n145\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-151427.log\nGPT1T_1L\n4\n2023-10-16-151427\n2023-10-16-151600\n151427\n151600\n173\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-152528.log\nGPT1T_1L\n4\n2023-10-16-152528\n2023-10-16-152640\n152528\n152640\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-175717.log\nGPT1T_1L\n4\n2023-10-16-175717\n2023-10-16-175829\n175717\n175829\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-180457.log\nGPT1T_1L\n4\n2023-10-16-180457\n2023-10-16-180605\n180457\n180605\n148\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183116.log\nGPT1T_1L\n4\n2023-10-16-183116\n2023-10-16-183216\n183116\n183216\n100\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183921.log\nGPT1T_1L\n4\n2023-10-16-183921\n2023-10-16-184033\n183921\n184033\n112\n\n\nforemans-nid008237-nhosts1-ngpu4-2023-10-16-215614.log\nGPT1T_1L\n4\n2023-10-16-215614\n2023-10-16-215815\n215614\n215815\n201\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-052944.log\nGPT1T_1L\n4\n2023-10-17-052944\n2023-10-17-053139\n52944\n53139\n195\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053529.log\nGPT1T_1L\n4\n2023-10-17-053529\n2023-10-17-053650\n53529\n53650\n121\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053910.log\nGPT1T_1L\n4\n2023-10-17-053910\n2023-10-17-054120\n53910\n54120\n210\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-054238.log\nGPT2_7B\n4\n2023-10-17-054238\n2023-10-17-054346\n54238\n54346\n108\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-060418.log\nGPT1T_1L\n4\n2023-10-17-060418\n2023-10-17-060600\n60418\n60600\n182\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-061514.log\nGPT1T_1L\n4\n2023-10-17-061514\n2023-10-17-061653\n61514\n61653\n139\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062102.log\nGPT1T_1L\n4\n2023-10-17-062102\n2023-10-17-062252\n62102\n62252\n150\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062445.log\nGPT1T_1L\n4\n2023-10-17-062445\n2023-10-17-062720\n62445\n62720\n275\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-064643.log\nGPT1T_1L\n8\n2023-10-17-064643\n2023-10-17-064848\n64643\n64848\n205\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-065806.log\nGPT1T_2L\n8\n2023-10-17-065806\n2023-10-17-070003\n65806\n70003\n4197\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-075152.log\nGPT1T_2L\n8\n2023-10-17-075152\n2023-10-17-075502\n75152\n75502\n350\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-080059.log\nGPT1T_2L\n8\n2023-10-17-080059\n2023-10-17-080434\n80059\n80434\n375\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-081404.log\nGPT1T_2L\n8\n2023-10-17-081404\n2023-10-17-081920\n81404\n81920\n516\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-090344.log\nGPT1T_1L\n4\n2023-10-17-090344\n2023-10-17-090714\n90344\n90714\n370\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-100759.log\nGPT1T_1L\n4\n2023-10-17-100759\n2023-10-17-100957\n100759\n100957\n198\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-182501.log\nGPT1T_1L\n16\n2023-10-17-182501\n2023-10-17-184001\n182501\n184001\n1500\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-193736.log\nGPT1T_1L\n16\n2023-10-17-193736\n2023-10-17-193856\n193736\n193856\n120\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-195432.log\nGPT1T_1L\n16\n2023-10-17-195432\n2023-10-17-195536\n195432\n195536\n104\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-201659.log\nGPT1T_2L\n16\n2023-10-17-201659\n2023-10-17-201823\n201659\n201823\n164\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-202949.log\nGPT1T_2L\n16\n2023-10-17-202949\n2023-10-17-203054\n202949\n203054\n105\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-205848.log\nGPT1T_1L\n16\n2023-10-17-205848\n2023-10-17-205952\n205848\n205952\n104\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213244.log\nGPT1T_1L\n32\n2023-10-17-213244\n2023-10-17-213406\n213244\n213406\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213558.log\nGPT1T_1L\n32\n2023-10-17-213558\n2023-10-17-213720\n213558\n213720\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-214900.log\nGPT1T_2L\n32\n2023-10-17-214900\n2023-10-17-214959\n214900\n214959\n59\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215201.log\nGPT1T_2L\n32\n2023-10-17-215201\n2023-10-17-215309\n215201\n215309\n108\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215612.log\nGPT1T_2L\n32\n2023-10-17-215612\n2023-10-17-215726\n215612\n215726\n114\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215938.log\nGPT1T_2L\n32\n2023-10-17-215938\n2023-10-17-220044\n215938\n220044\n4106\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110001.log\nGPT1T_4L\n32\n2023-10-18-110001\n2023-10-18-110143\n110001\n110143\n142\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110424.log\nGPT1T_8L\n32\n2023-10-18-110424\n2023-10-18-110550\n110424\n110550\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-110821.log\nGPT1T_8L\n16\n2023-10-18-110821\n2023-10-18-110952\n110821\n110952\n131\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-111345.log\nGPT1T_8L\n32\n2023-10-18-111345\n2023-10-18-111458\n111345\n111458\n113\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-112531.log\nGPT1T_16L\n64\n2023-10-18-112531\n2023-10-18-112728\n112531\n112728\n197\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-113119.log\nGPT1T_16L\n64\n2023-10-18-113119\n2023-10-18-113343\n113119\n113343\n224\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113131.log\nGPT1T_4L\n16\n2023-10-18-113131\n2023-10-18-113257\n113131\n113257\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113920.log\nGPT1T_4L\n16\n2023-10-18-113920\n2023-10-18-114157\n113920\n114157\n237\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-114549.log\nGPT1T_16L\n64\n2023-10-18-114549\n2023-10-18-114721\n114549\n114721\n172\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-114636.log\nGPT1T_16L\n64\n2023-10-18-114636\n2023-10-18-114805\n114636\n114805\n169\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-115808.log\nGPT1T_4L\n16\n2023-10-18-115808\n2023-10-18-120146\n115808\n120146\n4338\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-123039.log\nGPT1T_16L\n64\n2023-10-18-123039\n2023-10-18-123221\n123039\n123221\n182\n\n\nforemans-nid008389-nhosts2-ngpu8-2023-10-18-123135.log\nGPT1T_4L\n8\n2023-10-18-123135\n2023-10-18-123300\n123135\n123300\n165\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-123206.log\nGPT1T_4L\n16\n2023-10-18-123206\n2023-10-18-123352\n123206\n123352\n146\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-125022.log\nGPT1T_16L\n64\n2023-10-18-125022\n2023-10-18-125146\n125022\n125146\n124\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-122736.log\nGPT1T_8L\n32\n2023-10-22-122736\n2023-10-22-122844\n122736\n122844\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-123824.log\nGPT1T_8L\n32\n2023-10-22-123824\n2023-10-22-123945\n123824\n123945\n121\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-130148.log\nGPT1T_8L\n32\n2023-10-22-130148\n2023-10-22-130256\n130148\n130256\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-131746.log\nGPT1T_8L\n32\n2023-10-22-131746\n2023-10-22-131909\n131746\n131909\n163\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-132700.log\nGPT1T_8L\n32\n2023-10-22-132700\n2023-10-22-132817\n132700\n132817\n117\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-133459.log\nGPT1T_8L\n32\n2023-10-22-133459\n2023-10-22-133708\n133459\n133708\n249\n\n\nforemans-nid008380-nhosts4-ngpu16-2023-10-22-175049.log\nactCkpt_GPT25B\n16\n2023-10-22-175049\n2023-10-22-175230\n175049\n175230\n181\n\n\nforemans-nid008649-nhosts4-ngpu16-2023-10-22-192352.log\nGPT1T_4L\n16\n2023-10-22-192352\n2023-10-22-192530\n192352\n192530\n178\n\n\nforemans-nid008212-nhosts16-ngpu64-2023-10-23-081527.log\nGPT1T_8L\n64\n2023-10-23-081527\n2023-10-23-081702\n81527\n81702\n175\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-23-091436.log\nGPT1T_2L\n8\n2023-10-23-091436\n2023-10-23-091610\n91436\n91610\n174\n\n\nforemans-nid008197-nhosts32-ngpu128-2023-10-24-102617.log\nGPT1T_32L\n128\n2023-10-24-102617\n2023-10-24-102826\n102617\n102826\n209\n\n\nforemans-nid008192-nhosts64-ngpu256-2023-10-24-191748.log\nGPT1T_64L\n256\n2023-10-24-191748\n2023-10-24-192021\n191748\n192021\n273\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-24-201243.log\nGPT1T_128L\n512\n2023-10-24-201243\n2023-10-24-201629\n201243\n201629\n386\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-26-005401.log\nGPT1T_128L\n512\n2023-10-26-005401\n2023-10-26-005811\n5401\n5811\n410\n\n\nforemans-nid008192-nhosts32-ngpu128-2023-10-26-082710.log\nGPT1T_32L\n128\n2023-10-26-082710\n2023-10-26-083049\n82710\n83049\n339\n\n\nforemans-nid008585-nhosts2-ngpu8-2023-10-31-044203.log\nGPT1T_2L\n8\n2023-10-31-044203\n2023-10-31-044533\n44203\n44533\n330\n\n\nforemans-nid008272-nhosts4-ngpu16-2023-10-31-072717.log\nGPT1T_4L\n16\n2023-10-31-072717\n2023-10-31-073131\n72717\n73131\n414\n\n\nforemans-nid008221-nhosts8-ngpu32-2023-10-31-083055.log\nGPT1T_8L\n32\n2023-10-31-083055\n2023-10-31-083545\n83055\n83545\n490\n\n\nforemans-nid008196-nhosts16-ngpu64-2023-10-31-100336.log\nGPT1T_16L\n64\n2023-10-31-100336\n2023-10-31-100848\n100336\n100848\n512\n\n\nforemans-nid008285-nhosts2-ngpu8-2023-11-01-200430.log\nGPT1T_2L\n8\n2023-11-01-200430\n2023-11-01-200829\n200430\n200829\n399\n\n\nforemans-nid008193-nhosts8-ngpu32-2023-11-01-201702.log\nGPT1T_8L\n32\n2023-11-01-201702\n2023-11-01-202131\n201702\n202131\n429\n\n\nforemans-nid008240-nhosts16-ngpu64-2023-11-01-210454.log\nGPT1T_16L\n64\n2023-11-01-210454\n2023-11-01-211007\n210454\n211007\n553\n\n\nforemans-nid008321-nhosts2-ngpu8-2023-11-02-154438.log\nGPT1T_2L\n8\n2023-11-02-154438\n2023-11-02-154949\n154438\n154949\n511\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-11-04-001717.log\nGPT1T_128L\n512\n2023-11-04-001717\n2023-11-04-002124\n1717\n2124\n407"
  },
  {
    "objectID": "qmd/ezpz/ezpz.html#initialization-times",
    "href": "qmd/ezpz/ezpz.html#initialization-times",
    "title": "Starting Up Distributed Training",
    "section": "",
    "text": "Application Startup Time\n\n\n\n\n\n\nFrom Tanima:\n\nHi Sam and Corey,\nThanks for your comments on measuring the application start up time last week.\nTypically, we report the throughput performance after the start-up and warm-up during the “steady” state of the training.\nWe have a few follow-up questions so that we establish a methodology to address the issue brought up by Argonne.\n\nWe can set a few timestamps in the model scripts and job scripts used for the queue submission: Job script:\nTime stamp A:  \n&lt;actual python command using mpiexec&gt;\n\nInside the model script:  \nmain()  \nTimestamp B:  \n[...]\nTimestamp C:  \nFirst training steps and onwards.  \nBy startup time, do you mean measuring time difference between A and C or B and C?\n\n\n\n\nWill the measurement methodology be the same for distributed training?\nFor examples, we can measure the start-up time for the rank0?\n\n\n\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases?\nFor example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\nJob script:\nTime stamp A:\n&lt;actual python command using mpiexec&gt;\n\nTime stamp B:\n import torch\nTime stamp C\nimport IPEX\nTime stamp D\nEtc...\nIf you have any other scenarios, please feel free to suggest.\n\nThanks, Tanima.\n\n\n\n\n\n\nIn Measuring / Calculating Startup Time,I provide a summary of how the startup time is identified and calculated.\nI’m not sure exactly I understand\n\nWill the measurement methodology be the same for distributed training? For examples, we can measure the start-up time for the rank0?\n\nThe startup time is being measured for distributed training (logs only created on RANK = 0)\nI discuss in Minimal Working Example a minimal example that can be used to measure the startup times.\n\nThis is using a library I’ve been working on, ezpz that is designed to help simplify the process of setting up / initializing distributed training across many GPUs.\n\n\n\n\n\nThe startup timing was identified by parsing the logfiles from existing runs and calculating the difference \\delta t = t_{1} - t_{0},\n\nt_{0} is the time stamp at the very beginning of the shell script (defined here) which then launches mpiexec &lt;mpi-args&gt; python3 [...].\n\nt_{0} appears in the logfile as:\nJob started at: 2023-11-02-183323 on x3004c0s13b0n0\n\nt_{1} is identified as the timestamp associated with the completion of the first training step\n\nt_{1} appears in the logfile as:\n[2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n\nBelow is an example of the bash script use to parse the logfiles and identify these timestamps:\n  $ for f in $(tail -5 logfiles) ; do echo $f; cat $f | grep -E \"Job started|step=0\\,\" | uniq ; echo \"\\n\" ; done\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-183323.log\n  Job started at: 2023-11-02-183323 on x3004c0s13b0n0\n  [2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184240.log\n  Job started at: 2023-11-02-184240 on x3015c0s37b0n0\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184259.log\n  Job started at: 2023-11-02-184259 on x3015c0s37b0n0\n  [2023-11-02 18:43:23,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-184407.log\n  Job started at: 2023-11-02-184407 on x3004c0s13b0n0\n  [2023-11-02 18:44:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb2/logs/foremans-x3108c0s25b1n0-nhosts2-ngpu8-2023-11-02-192739.log\n  Job started at: 2023-11-02-192739 on x3108c0s25b1n0\n\n\n\n\n\n\n\n Startup Times (Perlmutter)\n\n\n\n\n\n\n\nTable 1: ?(caption)\n\n\n\n\n(a) Startup times on Perlmutter\n\n\n\n\n\n\n\n\n\n\n\n\n****\nmodel_size\nworld_size\nstart\nstop\nt0\nt1\ndt\n\n\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191101.log\nGPT1T_1L\n8\n2023-10-05-191101\n2023-10-05-191215\n191101\n191215\n114\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191400.log\nGPT1T_1L\n8\n2023-10-05-191400\n2023-10-05-191511\n191400\n191511\n111\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191707.log\nGPT1T_1L\n8\n2023-10-05-191707\n2023-10-05-191817\n191707\n191817\n110\n\n\nforemans-nid008553-nhosts2-ngpu8-2023-10-15-114506.log\nGPT1T_2L\n8\n2023-10-15-114506\n2023-10-15-114616\n114506\n114616\n110\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-133531.log\nGPT2_7B\n8\n2023-10-15-133531\n2023-10-15-133745\n133531\n133745\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-135041.log\nGPT2_7B\n8\n2023-10-15-135041\n2023-10-15-135255\n135041\n135255\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-140806.log\nGPT2_7B\n8\n2023-10-15-140806\n2023-10-15-141236\n140806\n141236\n430\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-143120.log\nGPT2_7B\n8\n2023-10-15-143120\n2023-10-15-143655\n143120\n143655\n535\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154337.log\nGPT2_7B\n8\n2023-10-15-154337\n2023-10-15-154446\n154337\n154446\n109\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154943.log\nGPT1T_1L\n8\n2023-10-15-154943\n2023-10-15-155317\n154943\n155317\n374\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-162315.log\nGPT1T_1L\n8\n2023-10-15-162315\n2023-10-15-162441\n162315\n162441\n126\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-180714.log\nGPT2_7B\n8\n2023-10-15-180714\n2023-10-15-180805\n180714\n180805\n91\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-181733.log\nGPT2_7B\n8\n2023-10-15-181733\n2023-10-15-181834\n181733\n181834\n101\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-182228.log\nGPT1T_1L\n8\n2023-10-15-182228\n2023-10-15-183031\n182228\n183031\n803\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-183345.log\nGPT1T_2L\n8\n2023-10-15-183345\n2023-10-15-183750\n183345\n183750\n405\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-184442.log\nGPT1T_2L\n8\n2023-10-15-184442\n2023-10-15-184727\n184442\n184727\n285\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-185952.log\nGPT1T_1L\n8\n2023-10-15-185952\n2023-10-15-190046\n185952\n190046\n4094\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-191508.log\nGPT2_7B\n8\n2023-10-15-191508\n2023-10-15-191608\n191508\n191608\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-192404.log\nGPT2_7B\n8\n2023-10-15-192404\n2023-10-15-192504\n192404\n192504\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193041.log\nGPT2_7B\n8\n2023-10-15-193041\n2023-10-15-193137\n193041\n193137\n96\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193448.log\nGPT2_7B\n8\n2023-10-15-193448\n2023-10-15-193540\n193448\n193540\n92\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-195802.log\nGPT1T_1L\n16\n2023-10-15-195802\n2023-10-15-195904\n195802\n195904\n102\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200019.log\nGPT2_7B\n16\n2023-10-15-200019\n2023-10-15-200258\n200019\n200258\n239\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200902.log\nGPT2_7B\n16\n2023-10-15-200902\n2023-10-15-201239\n200902\n201239\n337\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201524.log\nGPT2_7B\n16\n2023-10-15-201524\n2023-10-15-201612\n201524\n201612\n88\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201834.log\nGPT2_7B\n16\n2023-10-15-201834\n2023-10-15-201923\n201834\n201923\n89\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202402.log\nGPT2_7B\n16\n2023-10-15-202402\n2023-10-15-202501\n202402\n202501\n99\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202606.log\nGPT2_7B\n16\n2023-10-15-202606\n2023-10-15-202713\n202606\n202713\n107\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084033.log\nGPT1T_1L\n8\n2023-10-16-084033\n2023-10-16-084212\n84033\n84212\n179\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084628.log\nGPT1T_1L\n8\n2023-10-16-084628\n2023-10-16-084728\n84628\n84728\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-085401.log\nGPT1T_1L\n8\n2023-10-16-085401\n2023-10-16-085505\n85401\n85505\n104\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-090142.log\nGPT1T_1L\n8\n2023-10-16-090142\n2023-10-16-090305\n90142\n90305\n163\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-093404.log\nactCkpt_GPT13B\n8\n2023-10-16-093404\n2023-10-16-093504\n93404\n93504\n100\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-101437.log\nGPT1T_1L\n16\n2023-10-16-101437\n2023-10-16-101549\n101437\n101549\n112\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-101512.log\nGPT1T_1L\n16\n2023-10-16-101512\n2023-10-16-101615\n101512\n101615\n103\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102217.log\nactCkpt_GPT25B\n16\n2023-10-16-102217\n2023-10-16-102452\n102217\n102452\n235\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102750.log\nactCkpt_GPT25B\n16\n2023-10-16-102750\n2023-10-16-103243\n102750\n103243\n493\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-103113.log\nactCkpt_GPT25B\n16\n2023-10-16-103113\n2023-10-16-103237\n103113\n103237\n124\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104037.log\nactCkpt_GPT25B\n16\n2023-10-16-104037\n2023-10-16-104148\n104037\n104148\n111\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104819.log\nactCkpt_GPT25B\n16\n2023-10-16-104819\n2023-10-16-110002\n104819\n110002\n5183\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-110119.log\nactCkpt_GPT25B\n16\n2023-10-16-110119\n2023-10-16-110225\n110119\n110225\n106\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-113715.log\nactCkpt_GPT25B\n16\n2023-10-16-113715\n2023-10-16-113824\n113715\n113824\n109\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114236.log\nGPT1T_1L\n16\n2023-10-16-114236\n2023-10-16-114338\n114236\n114338\n102\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114610.log\nGPT1T_1L\n16\n2023-10-16-114610\n2023-10-16-114711\n114610\n114711\n101\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114819.log\nGPT1T_2L\n16\n2023-10-16-114819\n2023-10-16-114953\n114819\n114953\n134\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-131058.log\nGPT1T_2L\n16\n2023-10-16-131058\n2023-10-16-131203\n131058\n131203\n145\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-151427.log\nGPT1T_1L\n4\n2023-10-16-151427\n2023-10-16-151600\n151427\n151600\n173\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-152528.log\nGPT1T_1L\n4\n2023-10-16-152528\n2023-10-16-152640\n152528\n152640\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-175717.log\nGPT1T_1L\n4\n2023-10-16-175717\n2023-10-16-175829\n175717\n175829\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-180457.log\nGPT1T_1L\n4\n2023-10-16-180457\n2023-10-16-180605\n180457\n180605\n148\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183116.log\nGPT1T_1L\n4\n2023-10-16-183116\n2023-10-16-183216\n183116\n183216\n100\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183921.log\nGPT1T_1L\n4\n2023-10-16-183921\n2023-10-16-184033\n183921\n184033\n112\n\n\nforemans-nid008237-nhosts1-ngpu4-2023-10-16-215614.log\nGPT1T_1L\n4\n2023-10-16-215614\n2023-10-16-215815\n215614\n215815\n201\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-052944.log\nGPT1T_1L\n4\n2023-10-17-052944\n2023-10-17-053139\n52944\n53139\n195\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053529.log\nGPT1T_1L\n4\n2023-10-17-053529\n2023-10-17-053650\n53529\n53650\n121\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053910.log\nGPT1T_1L\n4\n2023-10-17-053910\n2023-10-17-054120\n53910\n54120\n210\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-054238.log\nGPT2_7B\n4\n2023-10-17-054238\n2023-10-17-054346\n54238\n54346\n108\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-060418.log\nGPT1T_1L\n4\n2023-10-17-060418\n2023-10-17-060600\n60418\n60600\n182\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-061514.log\nGPT1T_1L\n4\n2023-10-17-061514\n2023-10-17-061653\n61514\n61653\n139\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062102.log\nGPT1T_1L\n4\n2023-10-17-062102\n2023-10-17-062252\n62102\n62252\n150\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062445.log\nGPT1T_1L\n4\n2023-10-17-062445\n2023-10-17-062720\n62445\n62720\n275\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-064643.log\nGPT1T_1L\n8\n2023-10-17-064643\n2023-10-17-064848\n64643\n64848\n205\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-065806.log\nGPT1T_2L\n8\n2023-10-17-065806\n2023-10-17-070003\n65806\n70003\n4197\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-075152.log\nGPT1T_2L\n8\n2023-10-17-075152\n2023-10-17-075502\n75152\n75502\n350\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-080059.log\nGPT1T_2L\n8\n2023-10-17-080059\n2023-10-17-080434\n80059\n80434\n375\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-081404.log\nGPT1T_2L\n8\n2023-10-17-081404\n2023-10-17-081920\n81404\n81920\n516\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-090344.log\nGPT1T_1L\n4\n2023-10-17-090344\n2023-10-17-090714\n90344\n90714\n370\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-100759.log\nGPT1T_1L\n4\n2023-10-17-100759\n2023-10-17-100957\n100759\n100957\n198\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-182501.log\nGPT1T_1L\n16\n2023-10-17-182501\n2023-10-17-184001\n182501\n184001\n1500\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-193736.log\nGPT1T_1L\n16\n2023-10-17-193736\n2023-10-17-193856\n193736\n193856\n120\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-195432.log\nGPT1T_1L\n16\n2023-10-17-195432\n2023-10-17-195536\n195432\n195536\n104\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-201659.log\nGPT1T_2L\n16\n2023-10-17-201659\n2023-10-17-201823\n201659\n201823\n164\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-202949.log\nGPT1T_2L\n16\n2023-10-17-202949\n2023-10-17-203054\n202949\n203054\n105\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-205848.log\nGPT1T_1L\n16\n2023-10-17-205848\n2023-10-17-205952\n205848\n205952\n104\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213244.log\nGPT1T_1L\n32\n2023-10-17-213244\n2023-10-17-213406\n213244\n213406\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213558.log\nGPT1T_1L\n32\n2023-10-17-213558\n2023-10-17-213720\n213558\n213720\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-214900.log\nGPT1T_2L\n32\n2023-10-17-214900\n2023-10-17-214959\n214900\n214959\n59\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215201.log\nGPT1T_2L\n32\n2023-10-17-215201\n2023-10-17-215309\n215201\n215309\n108\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215612.log\nGPT1T_2L\n32\n2023-10-17-215612\n2023-10-17-215726\n215612\n215726\n114\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215938.log\nGPT1T_2L\n32\n2023-10-17-215938\n2023-10-17-220044\n215938\n220044\n4106\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110001.log\nGPT1T_4L\n32\n2023-10-18-110001\n2023-10-18-110143\n110001\n110143\n142\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110424.log\nGPT1T_8L\n32\n2023-10-18-110424\n2023-10-18-110550\n110424\n110550\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-110821.log\nGPT1T_8L\n16\n2023-10-18-110821\n2023-10-18-110952\n110821\n110952\n131\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-111345.log\nGPT1T_8L\n32\n2023-10-18-111345\n2023-10-18-111458\n111345\n111458\n113\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-112531.log\nGPT1T_16L\n64\n2023-10-18-112531\n2023-10-18-112728\n112531\n112728\n197\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-113119.log\nGPT1T_16L\n64\n2023-10-18-113119\n2023-10-18-113343\n113119\n113343\n224\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113131.log\nGPT1T_4L\n16\n2023-10-18-113131\n2023-10-18-113257\n113131\n113257\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113920.log\nGPT1T_4L\n16\n2023-10-18-113920\n2023-10-18-114157\n113920\n114157\n237\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-114549.log\nGPT1T_16L\n64\n2023-10-18-114549\n2023-10-18-114721\n114549\n114721\n172\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-114636.log\nGPT1T_16L\n64\n2023-10-18-114636\n2023-10-18-114805\n114636\n114805\n169\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-115808.log\nGPT1T_4L\n16\n2023-10-18-115808\n2023-10-18-120146\n115808\n120146\n4338\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-123039.log\nGPT1T_16L\n64\n2023-10-18-123039\n2023-10-18-123221\n123039\n123221\n182\n\n\nforemans-nid008389-nhosts2-ngpu8-2023-10-18-123135.log\nGPT1T_4L\n8\n2023-10-18-123135\n2023-10-18-123300\n123135\n123300\n165\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-123206.log\nGPT1T_4L\n16\n2023-10-18-123206\n2023-10-18-123352\n123206\n123352\n146\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-125022.log\nGPT1T_16L\n64\n2023-10-18-125022\n2023-10-18-125146\n125022\n125146\n124\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-122736.log\nGPT1T_8L\n32\n2023-10-22-122736\n2023-10-22-122844\n122736\n122844\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-123824.log\nGPT1T_8L\n32\n2023-10-22-123824\n2023-10-22-123945\n123824\n123945\n121\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-130148.log\nGPT1T_8L\n32\n2023-10-22-130148\n2023-10-22-130256\n130148\n130256\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-131746.log\nGPT1T_8L\n32\n2023-10-22-131746\n2023-10-22-131909\n131746\n131909\n163\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-132700.log\nGPT1T_8L\n32\n2023-10-22-132700\n2023-10-22-132817\n132700\n132817\n117\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-133459.log\nGPT1T_8L\n32\n2023-10-22-133459\n2023-10-22-133708\n133459\n133708\n249\n\n\nforemans-nid008380-nhosts4-ngpu16-2023-10-22-175049.log\nactCkpt_GPT25B\n16\n2023-10-22-175049\n2023-10-22-175230\n175049\n175230\n181\n\n\nforemans-nid008649-nhosts4-ngpu16-2023-10-22-192352.log\nGPT1T_4L\n16\n2023-10-22-192352\n2023-10-22-192530\n192352\n192530\n178\n\n\nforemans-nid008212-nhosts16-ngpu64-2023-10-23-081527.log\nGPT1T_8L\n64\n2023-10-23-081527\n2023-10-23-081702\n81527\n81702\n175\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-23-091436.log\nGPT1T_2L\n8\n2023-10-23-091436\n2023-10-23-091610\n91436\n91610\n174\n\n\nforemans-nid008197-nhosts32-ngpu128-2023-10-24-102617.log\nGPT1T_32L\n128\n2023-10-24-102617\n2023-10-24-102826\n102617\n102826\n209\n\n\nforemans-nid008192-nhosts64-ngpu256-2023-10-24-191748.log\nGPT1T_64L\n256\n2023-10-24-191748\n2023-10-24-192021\n191748\n192021\n273\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-24-201243.log\nGPT1T_128L\n512\n2023-10-24-201243\n2023-10-24-201629\n201243\n201629\n386\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-26-005401.log\nGPT1T_128L\n512\n2023-10-26-005401\n2023-10-26-005811\n5401\n5811\n410\n\n\nforemans-nid008192-nhosts32-ngpu128-2023-10-26-082710.log\nGPT1T_32L\n128\n2023-10-26-082710\n2023-10-26-083049\n82710\n83049\n339\n\n\nforemans-nid008585-nhosts2-ngpu8-2023-10-31-044203.log\nGPT1T_2L\n8\n2023-10-31-044203\n2023-10-31-044533\n44203\n44533\n330\n\n\nforemans-nid008272-nhosts4-ngpu16-2023-10-31-072717.log\nGPT1T_4L\n16\n2023-10-31-072717\n2023-10-31-073131\n72717\n73131\n414\n\n\nforemans-nid008221-nhosts8-ngpu32-2023-10-31-083055.log\nGPT1T_8L\n32\n2023-10-31-083055\n2023-10-31-083545\n83055\n83545\n490\n\n\nforemans-nid008196-nhosts16-ngpu64-2023-10-31-100336.log\nGPT1T_16L\n64\n2023-10-31-100336\n2023-10-31-100848\n100336\n100848\n512\n\n\nforemans-nid008285-nhosts2-ngpu8-2023-11-01-200430.log\nGPT1T_2L\n8\n2023-11-01-200430\n2023-11-01-200829\n200430\n200829\n399\n\n\nforemans-nid008193-nhosts8-ngpu32-2023-11-01-201702.log\nGPT1T_8L\n32\n2023-11-01-201702\n2023-11-01-202131\n201702\n202131\n429\n\n\nforemans-nid008240-nhosts16-ngpu64-2023-11-01-210454.log\nGPT1T_16L\n64\n2023-11-01-210454\n2023-11-01-211007\n210454\n211007\n553\n\n\nforemans-nid008321-nhosts2-ngpu8-2023-11-02-154438.log\nGPT1T_2L\n8\n2023-11-02-154438\n2023-11-02-154949\n154438\n154949\n511\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-11-04-001717.log\nGPT1T_128L\n512\n2023-11-04-001717\n2023-11-04-002124\n1717\n2124\n407"
  },
  {
    "objectID": "qmd/ezpz/ezpz.html#minimal-working-example",
    "href": "qmd/ezpz/ezpz.html#minimal-working-example",
    "title": "Starting Up Distributed Training",
    "section": "Minimal Working Example",
    "text": "Minimal Working Example\n\nAs for 3:\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases? For example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\n\n\nI’ve been working on a library to help simplify this:\n ezpz\nMinimal library that handles the initialization of distributed training\n\n  Working on Aurora, example:\n\nSetup / Install:\n# launch job\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=2:00:00 -l select=4 -I\n\n# load frameworks\n$ module use -a /soft/modulefiles ; module --ignore_cache load frameworks\n$ module load frameworks/.2023.12.15.001\n\n# install `ezpz`\n$ git clone https://github.com/saforem2/ezpz\n$ cd ezpz\n$ mkdir -p venvs/aurora/2023.12.15.001\n$ python3 -m venv venvs/aurora/2023.12.15.001 --system-site-packages\n$ source venvs/aurora/2023.12.15.001/bin/activate\n$ python3 -m pip install -e .\n\n# print job info and define `launch` alias\n$ source ezpz/src/ezpz/bin/savejobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s6b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s7b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c7s0b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\nLaunch with framework=pytorch, backend=DDP:\n# ----------------------------------------------------------\n# launch + startup on all workers with\n# • `framework` ∈ {`pytorch`, `tensorflow`}\n# • `backend` ∈ {`horovod`, `deepspeed`, `DDP`}\n# where `deepspeed` and `DDP` only available for `pytorch`\n# ----------------------------------------------------------\n$ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:243] - Using DDP for distributed training\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:34][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 1 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 2 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 3 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 4 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 0 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 5 / 47\n[2023-12-19 13:33:35][INFO][__main__.py:49] - {\n    \"_target_\": \"ezpz.configs.TrainConfig\",\n    \"framework\": \"pytorch\",\n    \"backend\": \"DDP\",\n    \"ds_config_path\": null,\n    \"port\": null,\n    \"seed\": null,\n    \"use_wandb\": true,\n    \"wandb_project_name\": null,\n    \"precision\": null,\n    \"ngpus\": null\n}\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 9 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 10 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 11 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 7 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 8 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 6 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 12 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 13 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 14 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 15 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 18 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 19 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 20 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 21 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 22 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 23 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 24 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 25 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 26 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 27 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 30 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 16 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 17 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 28 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 32 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 33 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 36 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 37 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 38 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 39 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 43 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 46 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 29 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 47 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 31 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 34 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 35 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 42 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 41 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 44 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 45 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 40 / 47\n[2023-12-19 13:33:47][INFO][dist.py:415] - Setting up wandb from rank: 0\n[2023-12-19 13:33:47][INFO][dist.py:416] - Using: WB PROJECT: ezpz\n[2023-12-19 13:33:58][INFO][dist.py:448] - W&B RUN: [flowing-wood-8](https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5)\n[2023-12-19 13:33:58][INFO][dist.py:490] - Running on x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][dist.py:506] - Reading hosts from /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][__main__.py:57] - Output dir: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17\n[2023-12-19 13:33:58][CRITICAL][dist.py:519] - 🚀 flowing-wood-8\n[2023-12-19 13:33:58][CRITICAL][dist.py:520] - 🔗 https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5\n[2023-12-19 13:33:58][CRITICAL][dist.py:521] - 📂/: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/wandb/run-20231219_133354-uya29gm5/files\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/ezpz-pt-DDP-xpu.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/__main__.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/main_debug.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-16/__main__.log to W&B artifact..."
  },
  {
    "objectID": "qmd/slides.html",
    "href": "qmd/slides.html",
    "title": "Recent Talks",
    "section": "",
    "text": "\\hspace{1pt} Exascale Science on Aurora @ Intel oneAPI Workshop @ UIC (10/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} LLMs on Polaris @ ALCF Hands On HPC Workshop (10/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Scaling LLMs for Science and Ongoing Collaborations @ Data-Intensive Computing and AI/ML at Scale (08/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} MLMC: Machine Learning Monte Carlo @ Lattice 2023 (07/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Generative Modeling and Efficient Sampling @ PASC23 (07/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Efficient Sampling for Lattice Gauge Theory @ Deep Fridays @ U. Bologna (04/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Large Scale Training @ Introduction to AI4Science on Supercomputers (ALCF) (11/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Hyperparameter Management @ ALCF Simulation, Data, and Learning Workshop (10/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Statistical Learning @ ATPESC 2022 (08/2022)\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Scientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Machine Learning in HEP @ UNC Greensboro (03/2022)\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Older Slides\n\n\n\n\n\nOlder slides (pre-2022) can be found here"
  },
  {
    "objectID": "qmd/slides.html#fa-regular-calendar-2023",
    "href": "qmd/slides.html#fa-regular-calendar-2023",
    "title": "Recent Talks",
    "section": "",
    "text": "\\hspace{1pt} Exascale Science on Aurora @ Intel oneAPI Workshop @ UIC (10/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} LLMs on Polaris @ ALCF Hands On HPC Workshop (10/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Scaling LLMs for Science and Ongoing Collaborations @ Data-Intensive Computing and AI/ML at Scale (08/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} MLMC: Machine Learning Monte Carlo @ Lattice 2023 (07/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Generative Modeling and Efficient Sampling @ PASC23 (07/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Efficient Sampling for Lattice Gauge Theory @ Deep Fridays @ U. Bologna (04/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "qmd/slides.html#fa-regular-calendar-2022",
    "href": "qmd/slides.html#fa-regular-calendar-2022",
    "title": "Recent Talks",
    "section": "",
    "text": "\\hspace{1pt} Large Scale Training @ Introduction to AI4Science on Supercomputers (ALCF) (11/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Hyperparameter Management @ ALCF Simulation, Data, and Learning Workshop (10/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Statistical Learning @ ATPESC 2022 (08/2022)\n\n\n\n\n\n\n📕 accompanying notebook\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Scientific Data Science: An Emerging Symbiosis @ ANL (05/2022)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{1pt} Machine Learning in HEP @ UNC Greensboro (03/2022)\n\n\n\n\n\n\nMachine Learning in HEP, at UNC Greensboro, March 2022\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;"
  },
  {
    "objectID": "qmd/slides.html#bi-hourglass-split-older",
    "href": "qmd/slides.html#bi-hourglass-split-older",
    "title": "Recent Talks",
    "section": "",
    "text": "\\hspace{1pt} Older Slides\n\n\n\n\n\nOlder slides (pre-2022) can be found here"
  },
  {
    "objectID": "qmd/projects.html",
    "href": "qmd/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n l2hmc-qcd\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2023,\n  author = {Foreman, Sam},\n  title = {Personal {Website}},\n  date = {2023-10-05},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2023. “Personal Website.” October 5, 2023. https://samforeman.me."
  },
  {
    "objectID": "qmd/features.html",
    "href": "qmd/features.html",
    "title": "Sam Foreman",
    "section": "",
    "text": "Features\n\n\n\n\nBlog\nLatest from the blog\n\n\n\n\n  \n\n\n\n\nHow to make dope slides\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nSam Foreman \n\n\n\n\n\n\nNo matching items\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{foreman2023,\n  author = {Foreman, Sam},\n  title = {Personal {Website}},\n  date = {2023-10-30},\n  url = {https://samforeman.me},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForeman, Sam. 2023. “Personal Website.” October 30, 2023.\nhttps://samforeman.me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "👋\nI received my PhD in Physics from the University of Iowa in 2019 and my thesis was on Learning Better Physics: A Machine Learning Approach to Lattice Gauge Theory. Prior to this, I completed two bachelors degrees (Engineering Physics and Applied Mathematics, 2015) from The University of Illinois at Urbana-Champaign. My undergraduate dissertation was titled Energy Storage in Quantum Resonators and was supervised by Professor Alfred Hübler within the Center for Complex Systems Research at UIUC.\nAs a member of the Data Science Group at ALCF, I work on:"
  },
  {
    "objectID": "index.html#fa-regular-newspaper-recent-work",
    "href": "index.html#fa-regular-newspaper-recent-work",
    "title": "",
    "section": " Recent Work",
    "text": "Recent Work\n\nDeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery […], NeurIPS 2023 AI For Science Workshop, Oct 2023\n\n DeepSpeed4Science.ai Blog Post\n Loooooooong Sequence Lengths\n\nA Comprehensive Performance Study of Large Language Models on Novel AI Accelerators, M. Emani, S. Foreman, et al., IPDPS 2024, Oct 2023\nExploratory Analysis of Climate Data with ClimRR, S. Foreman, Intro to HPC Bootcamp @ NERSC, August 7, 2023\n\nGenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics, M. Zvyagin et. al., Oct 2022\n\n ACM Gordon Bell Special Prize for HPC-Based COVID-19 Research\n\n\nLattice QCD and Particle Physics, A.S. Kronfeld et al., July 15, 2022\n\nApplications of ML to Lattice QFT, arXiv:2202.05838, D. Boyda, S. Calí, S. Foreman, et al., Feb 2022\n\nLeapFrogLayers: Trainable Framework for Effective Sampling, S. Foreman, X.Y. Jin, J.C. Osborn, Lattice, 2021\n\nHMC with Normalizing Flows, slides, S. Foreman et al., Lattice, 2021\n\nDeep Learning Hamiltonian Monte Carlo (+ poster), S. Foreman, X.Y. Jin, & J.C. Osborn, @ SimDL Workshop @ ICLR, 2021\n\nMachine Learning and Neural Networks for Field Theory, S. Foreman, X.Y. Jin, & J.C. Osborn, SnowMass, 2020\n\nExamples of renormalization group transformations for image sets, S. Foreman et al., Physical Review E., 2018\n\nRG inspired Machine Learning for lattice field theory S. Foreman et al., arXiv:1710.02079, 2017\n\nLarge Energy Density in Three-Plate Nanocapacitors due to Coulomb Blockade, S. Foreman et al., J. Appl. Phys, 2018"
  },
  {
    "objectID": "index.html#fa-solid-person-chalkboard-recent-talks",
    "href": "index.html#fa-solid-person-chalkboard-recent-talks",
    "title": "",
    "section": " Recent Talks",
    "text": "Recent Talks\n\nMLMC: Machine Learning Monte Carlo for Lattice Gauge Theory, at Lattice 2023, July 2023\nGenerative Modeling and Efficient Sampling, at PASC23, July 2023\nEfficient Sampling for Lattice Gauge Theory, at Deep Fridays @ U. Bologna, April 2023\nLarge Scale Training, at Introduction to AI-driven Science on Supercomputers: A Student Training Series, November 2022\nHyperparameter Management, at 2022 ALCF Simulation, Data, and Learning Workshop, October 2022\nStatistical Learning, at ATPESC 2022, August 2022 📕 accompanying notebook\nScientific Data Science: An Emerging Symbiosis, at Argonne National Laboratory, May 2022\nMachine Learning in HEP, at UNC Greensboro, March 2022\nAccelerated Sampling Methods for Lattice Gauge Theory, at BNL-HET& RBRC Joint Workshop “DWQ @ 25”, Dec 2021\nTraining Topological Samplers for Lattice Gauge Theory, ML4HEP, on and off the Lattice @ ECT* Trento, Sep 2021\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\nDeep Learning HMC for Improved Gauge Generation to the Machine Learning Techniques in Lattice QCD Workshop, 2021\nMachine Learning for Lattice QCD at the University of Iowa, 2020\nMachine learning inspired analysis of the Ising model transition to Lattice, 2018\nMachine Learning Analysis of Ising Worms at Brookhaven National Laboratory, 2017"
  },
  {
    "objectID": "index.html#fa-brands-github-alt-active-projects",
    "href": "index.html#fa-brands-github-alt-active-projects",
    "title": "",
    "section": " Active Projects",
    "text": "Active Projects\n\n\n\n\n\n\n GitHub Stats (saforem2):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nl2hmc-qcd\n\n\n\n\n\n\n\n GitHub repo"
  },
  {
    "objectID": "index.html#fa-solid-building-experience",
    "href": "index.html#fa-solid-building-experience",
    "title": "",
    "section": " Experience",
    "text": "Experience\n\n\n\n\nAssistant Computational Scientist\nALCF\n2022\n–\n\n\n\nPostdoc\nALCF\n2019\n2022\n\n\n\nGraduate Researcher\nANL\n2018\n2019"
  },
  {
    "objectID": "index.html#fa-solid-graduation-cap-education",
    "href": "index.html#fa-solid-graduation-cap-education",
    "title": "",
    "section": " Education",
    "text": "Education\n\n\n\n\nPhD\nPhysics\nUniversity of Iowa\n2019\n\n\nB.Sc\nPhysics\nUIUC\n2015\n\n\nB.Sc\nMath\nUIUC\n2015"
  },
  {
    "objectID": "index.html#fa-solid-bullhorn-events",
    "href": "index.html#fa-solid-bullhorn-events",
    "title": "",
    "section": " Events",
    "text": "Events\n\nOrganizer for Machine Learning and Quantum Computing for Earth Sciences at 17th U. S. National Congress on Computational Mechanics, July 2023\nOrganizer for SC23 Workshop: High Performance Python for Science at Scale (HPPSS), November 2023"
  },
  {
    "objectID": "index.html#fa-solid-hourglass-end-appendix",
    "href": "index.html#fa-solid-hourglass-end-appendix",
    "title": "",
    "section": " Appendix",
    "text": "Appendix\n\n\n\n\n\n\n Status\n\n\n\n\n\n\n\nLast Updated: 12/30/2023 @ 00:51:09"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMostly getting supercomputers to stop yelling at each other .↩︎\n⚡ powered by ezpz↩︎\nForked from   saforem2/opinionated↩︎"
  },
  {
    "objectID": "qmd/dsblog.html",
    "href": "qmd/dsblog.html",
    "title": "Loooooooong Sequence Lengths",
    "section": "",
    "text": "Figure 1: This work was done as part of the DeepSpeed4Science project, in collaboration with Microsoft.\nThe new Megatron-DeepSpeed release contains a variety of improvements / optimizations to enable pre-training Transformer based architectures with significantly longer sequences than was previously possible."
  },
  {
    "objectID": "qmd/dsblog.html#deepspeed4science-092023",
    "href": "qmd/dsblog.html#deepspeed4science-092023",
    "title": "Loooooooong Sequence Lengths",
    "section": "DeepSpeed4Science (09/2023)",
    "text": "DeepSpeed4Science (09/2023)\n\nNew Features\n\nEnabled Megatron-LM’s sequence parallel.\nEnabled rotary positional embedding.\nEnabled FlashAttention v1 and v2.\nEnabled new fused kernels from NVIDIA.\n\n\n\nNew optimizations\n\nEnabled attention map memory optimization, where we first generated attention mask on CPU memory and then moved it into GPU memory to avoid out-of-memory errors when training with very large sequence lengths.\nPosition embedding partitioning, where we split weights of position encoding across all GPUs when enabling sequence parallel to further reduce the memory footprint.\n\n\n\nInitial Results\n\n\nTable 1: Long sequence length support1 from microsoft/Megatron-DeepSpeed\n\n\nSequence Length\nOld Megatron-DeepSpeed (TFLOPS)\nNew Megatron-DeepSpeed (TFLOPS)\n\n\n\n\n2k\n25\n68\n\n\n4k\n28\n80\n\n\n8k\nOOM\n86\n\n\n16k\nOOM\n92\n\n\n32k\nOOM\n100\n\n\n64k\nOOM\n106\n\n\n128k\nOOM\n119\n\n\n256k\nOOM\n94\n\n\n\n\n\n\nFailed to download font: Source Sans Pro, skipping!\nFailed to download font: Titillium WebRoboto Condensed, skipping!\n\n\n\n\nData\ngpus = ('32', '64', '128')\n\ncolors = {\n    'Old Megatron-DS': '#FF5252',\n    'Megatron-LM': '#76b900',\n    'New Megatron-DS':  '#1A8FFF',\n}\n\ndata = {\n    '25B': {\n        'Old Megatron-DS': np.array([36, 42, 42]),\n        'Megatron-LM': np.array([26, 48, 52]),\n        'New Megatron-DS': np.array([192, 448, 512]),\n    },\n    '33B': {\n        'Old Megatron-DS': np.array([28, 32, 32]),\n        'Megatron-LM': np.array([14, 46, 52]),\n        'New Megatron-DS': np.array([128, 384, 448]),\n    },\n}\n\n\n\n\n\n\nMake the plots\nx = np.arange(len(gpus))\nwidth = 0.25\nmultiplier = 0\n\noutdir = Path(os.getcwd()).joinpath('assets')\noutdir.mkdir(exist_ok=True, parents=True)\n\nimprovement = {}\nfor idx, (model_size, d) in enumerate(data.items()):\n    multiplier = 0\n    figure, axes = plt.subplots(figsize=(7.5, 4))\n    fig = plt.gcf()\n    ax = plt.gca()\n    for label, value in d.items():\n        offset = width * multiplier\n        rects = ax.barh(\n          x + offset,\n          value,\n          width,\n          label=label,\n          color=colors[label],\n          alpha=0.8\n        )\n        ax.bar_label(\n          rects,\n          padding=3,\n          color=colors[label],\n          family='monospace',\n          weight='bold'\n        )\n        multiplier += 1\n    ax.set_ylabel(\n        'GPUs',\n        fontsize=18,\n        family='sans-serif',\n        loc='center',\n    )\n    ax.set_yticks(x + width, gpus)\n    plt.figtext(\n        0.005, 0.93, f\"{model_size}\", fontsize=24, fontweight='bold', ha='left'\n    )\n    ax.set_xlabel(\n        'Sequence Length (k)', fontsize=18, loc='center'\n    )\n    ax.legend(\n        bbox_to_anchor=(0.005, 1.04, 0.99, .098),\n        alignment='center',\n        edgecolor=\"#83838320\",\n        frameon=True,\n        ncols=3,\n        fontsize=13,\n        mode=\"expand\",\n        borderaxespad=0.01\n    )\n    save_figure(fname=f'{model_size}', outdir=outdir)\n    _ = plt.show()\n\n\n\n\n\n\nGPT-25B Model\n\n\n\n\n\nGPT-33B Model\n\n\n\n\n\nFigure 2: Pre-training with long sequence support across different model sizes and numbers of GPUs. In each case, the new (current) implementation significantly outperforms both NVIDIA/Megatron-LM as well as our previous implementation."
  },
  {
    "objectID": "qmd/dsblog.html#installation",
    "href": "qmd/dsblog.html#installation",
    "title": "Loooooooong Sequence Lengths",
    "section": "Installation",
    "text": "Installation\n\nUsing install.sh\n\n\n\n\n\n\nInstallation\n\n\n\n\n\nImportant To install, simply:\ngit clone https://github.com/ramanthanlab/GenSLM/\ncd GenSLM/examples/long-sequences/\n./install.sh\nExplicitly, ./install.sh will:\n\nAutomatically create a virtual environment on top of the latest conda module\nInstall (+ update2) / build all the required dependencies into this virtual environment\n\n\n\n\n\n\nStep-by-Step\nFor completeness, we describe below the steps for installing and building each of the dependencies.\n\nClone GitHub repo:\ngit clone https://github.com/ramanthanlab/GenSLM\nLoad conda module:\n\nThetaGPU:\n# ThetaGPU:\nif [[ \"$(hostname)==theta*\" ]]; then\n    export MACHINE=\"ThetaGPU\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-11\n    conda activate base\nfi\nPolaris:\n# Polaris:\nif [[ \"$(hostname)==x3*\" ]]; then\n    export MACHINE=\"Polaris\"\n    export CONDA_DATE=\"2023-01-10\"\n    module load conda/2023-01-10-unstable\n    conda activate base\nfi\n\nSetup Virtual Environment3:\ncd ./genslm/examples/long-sequences\n# create a new virtual environment\nmkdir -p \"venvs/${MACHINE}/${CONDA_DATE}\"\npython3 -m venv \"venvs/${MACHINE}/${CONDA_DATE}\" --system-site-packages\nsource \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\nCreate a new folder (genslm/examples/long-sequences/deps/${MACHINE}) where we’ll installing dependencies locally:\nmkdir -p \"deps/${MACHINE}\"\ncd \"deps/${MACHINE}\"\n\n\nDependencies\nWe provide below the details needed to install each of the required dependencies.\n\n\n saforem2/ezpz\n\n\n saforem2/ezpz\npip install -e \"git+https://github.com/saforem2/ezpz.git#egg=ezpz\"\n\n\n\n\n Microsoft/DeepSpeed\n\n\n Microsoft/DeepSpeed\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\npython3 -m pip install -e .\n\n\n\n\n Microsoft/Megatron-DeepSpeed\n\n\n Microsoft/Megatron-DeepSpeed:\ngit clone https://github.com/microsoft/Megatron-DeepSpeed.git\n\n\n\n\n NVIDIA/apex\n\n\n NVIDIA/apex\ngit clone https://github.com/NVIDIA/apex\ncd ../apex/\npip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" -e ./\n\n\n\n\n pybind/PyBind11\n\n\n pybind/PyBind11\npip install pybind11\n\n\n\n\n Dao-AILab/flash-attention\n\n\n Dao-AILab/flash-attention:\n\n\n\n\n\n\nFlash Attention\n\n\n\n\n\n\nThe new release supports three different implementations of FlashAttention: (v1.0.4, v2.x, triton)\nFlashAttention v2.x may have numerical instability issues. For the best performance, we recommend using FlashAttention + Triton\n\n\n\n\n\nv1.0.4:\npython3 -m pip install flash-attn==1.0.4\nv2.x:\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention\npython3 setup.py install\nopenai/triton:\ngit clone -b legacy-backend https://github.com/openai/triton\ncd triton/python\npython3 -m pip install cmake\npython3 -m pip install ."
  },
  {
    "objectID": "qmd/dsblog.html#running",
    "href": "qmd/dsblog.html#running",
    "title": "Loooooooong Sequence Lengths",
    "section": "Running",
    "text": "Running\nThe ALCF/ directory contains shell scripts for setting up the environment and specifying the options to be used when launching.\nVarious options can be specified dynamically at runtime by setting them in your environment, e.g.:\nMODEL_SIZE_KEY=\"GPT25B\" SEQ_LEN=128000 USE_FLASH_ATTN=1 MICRO_BATCH=1 GAS=1 SP_TYPE=\"megatron\" ZERO_STAGE=1 ./ALCF/train-gpt3.sh\nExplicitly:\n\nALCF/train-gpt3.sh: Main entry point for training\n\nThis script will automatically source the rest of the required ALCF/*.sh scripts below\n\nALCF/models.sh: Contains some example model architectures for GPT3-style models\nALCF/args.sh: Logic for parsing / setting up runtime options for Megatron and DeepSpeed\nALCF/setup.sh: Locate and activate virtual environment to be used, ensure MPI variables are set properly\nALCF/launch.sh: Identify available resources and build the command to be executed\n\ni.e. figure out how many: {nodes, GPUs per node, GPUs total}, to pass to mpi{run,exec}\nthen, use this to build mpiexec &lt;mpiexec-args&gt; python3 pretrain_gpt.py"
  },
  {
    "objectID": "qmd/dsblog.html#zero-offloading",
    "href": "qmd/dsblog.html#zero-offloading",
    "title": "Loooooooong Sequence Lengths",
    "section": "ZeRO Offloading",
    "text": "ZeRO Offloading\n🚀 W&B Report: Looooooooong Sequences\nThese newly introduced optimizations, in combination with ZeRO-Offload allows us to go even further.\nBy employing ZeRO-Offloading, we are able to free up additional memory which can be used for even longer sequences.\nThough work is still ongoing, this is a promising direction that will allow us to consider significantly larger genomes than previously possible.\nWe use Weights & Biases to track these experiments, and have aggregated our initial results in the W&B Report below.\nWe can evaluate the performance of our model by looking at two different metrics for throughput: samples_per_sec and TFLOPS.\nExplicitly, we see that we are able to scale up to significantly longer sequences (420k / 128k ~ 3.3x) with only a minimal impact on throughput performance (81 / 105 ~ 77\\%)4.\n\n\nTable 2: Impact on TFLOPS as a function of increasing sequence length. Table from: throughput/TFLOPS\n\n\nName\nSequence Length (k)\n(seq_len / min_seq_len)\nTFLOPS\nTFLOPS (% of peak)\n\n\n\n\nGPT25B\n420\n3.28125\n81.77225\n77.867\n\n\nGPT25B\n400\n3.125\n90.62\n86.297\n\n\nGPT25B\n360\n2.8125\n81.6325\n77.7348\n\n\nGPT25B\n360\n2.8125\n82.6824\n78.7346\n\n\nGPT25B\n192\n1.5\n115.8228\n110.2927\n\n\nGPT25B\n128\n1\n106.672\n101.5788\n\n\nGPT25B\n128\n1\n105.014\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Weights & Biases Report"
  },
  {
    "objectID": "qmd/dsblog.html#footnotes",
    "href": "qmd/dsblog.html#footnotes",
    "title": "Loooooooong Sequence Lengths",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe described experiments were performed on 4 NVIDIA DGX A100-40GB nodes, all using TPSIZE=32[^tpsize], connected through 8 HDR InfiniBand (200Gb/s per HDR).↩︎\n\ndeepspeed-0.10.3\npytorch==2.0.0+cu118\n\n↩︎\nWhere \"${MACHINE}\" \\in {\"ThetaGPU\", \"Polaris\"} and \"${CONDA_DATE}\" \\in {\"2023-01-10\", \"2023-01-11\"}↩︎\nthroughput/TFLOPS↩︎"
  },
  {
    "objectID": "qmd/posts.html",
    "href": "qmd/posts.html",
    "title": "Listing Example",
    "section": "",
    "text": "Intro to HPC Bootcamp @ Nersc: Climate Analysis with ClimRR (Foreman 2023)\nl2hmc-qcd \n\nYou can review the following documents for additional information:\n\n\n\n\n\n\n\n\nLatest from the blog\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 22, 2023\n\n\nStarting Up Distributed Training\n\n\nSam Foreman \n\n\n\n\nNov 2, 2023\n\n\nRecent Talks\n\n\nSam Foreman \n\n\n\n\nOct 31, 2023\n\n\nLoooooooong Sequence Lengths\n\n\nSam Foreman \n\n\n\n\nOct 30, 2023\n\n\nSam Foreman\n\n\nSam Foreman \n\n\n\n\nOct 5, 2023\n\n\nHow to make dope slides\n\n\nSam Foreman \n\n\n\n\nOct 5, 2023\n\n\nProjects\n\n\nSam Foreman \n\n\n\n\nOct 5, 2023\n\n\n(Less) Recent Talks\n\n\nSam Foreman \n\n\n\n\n\n\nNo matching items\n\n\n\n\nBlog\n\n\n\n\nLearn more about Quarto here."
  },
  {
    "objectID": "qmd/posts.html#external-links",
    "href": "qmd/posts.html#external-links",
    "title": "Listing Example",
    "section": "",
    "text": "Intro to HPC Bootcamp @ Nersc: Climate Analysis with ClimRR (Foreman 2023)\nl2hmc-qcd \n\nYou can review the following documents for additional information:\n\n\n\n\n\n\n\n\nLatest from the blog\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 22, 2023\n\n\nStarting Up Distributed Training\n\n\nSam Foreman \n\n\n\n\nNov 2, 2023\n\n\nRecent Talks\n\n\nSam Foreman \n\n\n\n\nOct 31, 2023\n\n\nLoooooooong Sequence Lengths\n\n\nSam Foreman \n\n\n\n\nOct 30, 2023\n\n\nSam Foreman\n\n\nSam Foreman \n\n\n\n\nOct 5, 2023\n\n\nHow to make dope slides\n\n\nSam Foreman \n\n\n\n\nOct 5, 2023\n\n\nProjects\n\n\nSam Foreman \n\n\n\n\nOct 5, 2023\n\n\n(Less) Recent Talks\n\n\nSam Foreman \n\n\n\n\n\n\nNo matching items\n\n\n\n\nBlog\n\n\n\n\nLearn more about Quarto here."
  },
  {
    "objectID": "qmd/slides-older.html",
    "href": "qmd/slides-older.html",
    "title": "(Less) Recent Talks",
    "section": "",
    "text": "\\hspace{2pt} Accelerated Sampling Methods for Lattice Gauge Theory, @ BNL / DWQ @ 25 (12/2021)\n\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} Training Topological Samplers for Lattice Gauge Theory @ ML4HEP, ECT* Trento (09/2021)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} l2hmc-qcd @ MIT Lattice Group Seminar (2021)\n\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} Deep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop (2021)\n\n\n\n\n\n\nDeep Learning HMC for Improved Gauge Generation to the Machine Learning Techniques in Lattice QCD Workshop, 2021"
  },
  {
    "objectID": "qmd/slides-older.html#fa-regular-calendar-2021",
    "href": "qmd/slides-older.html#fa-regular-calendar-2021",
    "title": "(Less) Recent Talks",
    "section": "",
    "text": "\\hspace{2pt} Accelerated Sampling Methods for Lattice Gauge Theory, @ BNL / DWQ @ 25 (12/2021)\n\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} Training Topological Samplers for Lattice Gauge Theory @ ML4HEP, ECT* Trento (09/2021)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} l2hmc-qcd @ MIT Lattice Group Seminar (2021)\n\n\n\n\n\n\nl2hmc-qcd at the MIT Lattice Group Seminar, 2021\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} Deep Learning HMC for Improved Gauge Generation @ ML in LQCD Workshop (2021)\n\n\n\n\n\n\nDeep Learning HMC for Improved Gauge Generation to the Machine Learning Techniques in Lattice QCD Workshop, 2021"
  },
  {
    "objectID": "qmd/slides-older.html#fa-regular-calendar-2020",
    "href": "qmd/slides-older.html#fa-regular-calendar-2020",
    "title": "(Less) Recent Talks",
    "section": " 2020",
    "text": "2020\n\n\n\n\n\n\n \\hspace{2pt} Machine Learning for Lattice QCD (2020)\n\n\n\n\n\n\nMachine Learning for Lattice QCD at the University of Iowa, 2020\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} Machine Learning Inspired Analysis of Ising Model @ Lattice (2018)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\n\n\n\n \\hspace{2pt} Machine Learning Analysis of Ising Worms (2017)\n\n\n\n\n\n\nMachine Learning Analysis of Ising Worms at Brookhaven National Laboratory, 2017"
  },
  {
    "objectID": "qmd/blog/quarto-reveal.html",
    "href": "qmd/blog/quarto-reveal.html",
    "title": "How to make dope slides",
    "section": "",
    "text": "By far the most common question I get after I give a talk is, “how did you make those slides?”\n\n\n\n\n\n\n \\hspace{2pt} MLMC: Machine Learning Monte Carlo @ Lattice 2023 (07/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\nAfter promising on twitter, I decided to make a blog post about how I make my slides.\n\nHonestly 99% of the work is done automatically by Quarto.\nI’ve made a GitHub repo  saforem2/slides-template that contains the theme + style I use for my slides.\n\n\n\n\n\n\n \\hspace{2pt} Slides Template (08/2023)\n\n\n\n\n\n\n&lt;p&gt;\nYour browser does not support iframes.\n&lt;/p&gt;\n\n\n\n\n\n\n\nReveal Themes\nUsing Pandoc fenced divs\nSlidecraft 101: Colors and Fonts\nBeautiful Reports and Presentations with Quarto\n Quarto Clean Theme"
  },
  {
    "objectID": "qmd/blog/quarto-reveal.html#references",
    "href": "qmd/blog/quarto-reveal.html#references",
    "title": "How to make dope slides",
    "section": "",
    "text": "Reveal Themes\nUsing Pandoc fenced divs\nSlidecraft 101: Colors and Fonts\nBeautiful Reports and Presentations with Quarto\n Quarto Clean Theme"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n saforem2/ezpz\n argonne-lcf/Megatron-DeepSpeed\n saforem2/wordplay [web]\n\n  saforem2/nanoGPT\n\n saforem2/enrich\n\n\n\n saforem2/ambivalent [web]\n\n saforem2/opinionated\n\n saforem2/l2hmc-qcd [web]\n saforem2/climate-analysis [web]\n saforem2/Megatron-DS-Benchmarking"
  },
  {
    "objectID": "index.html#fa-brands-github-projects",
    "href": "index.html#fa-brands-github-projects",
    "title": "",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n saforem2/ezpz\n\nDistributed training, ezpz.\n\n argonne-lcf/Megatron-DeepSpeed\n\nMegatron-LM + DeepSpeed, for the largest of large language models.\n\n saforem2/wordplay [web]\n\nBuilt from my  fork [saforem2/nanoGPT] with support for2\n{ 🤗 datasets, DeepSpeed }\n\n argonne-lcf/ai-science-training-series [web]\n\nStudent training series on AI-driven Science on Supercomputers\n\n\n\n\n saforem2/enrich\n\nPython’s logging, with Rich\n\n saforem2/ambivalent [web]\n\nMinimal, beautiful (+ highly-customizable) styles for Matplotlib3.\n\n saforem2/l2hmc-qcd [web]\n\nApplication of the L2HMC algorithm to simulations in lattice QCD.\n\n saforem2/climate-analysis [web]\n\nClimate Analysis project using ClimRR data"
  }
]